{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrZgAz-Cmreh",
        "outputId": "a24171bc-4cac-413d-debf-0e7d16455173"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Dataset URL: https://www.kaggle.com/datasets/adityajn105/flickr8k\n",
            "License(s): CC0-1.0\n",
            "Downloading flickr8k.zip to /content\n",
            " 99% 1.02G/1.04G [00:11<00:00, 90.8MB/s]\n",
            "100% 1.04G/1.04G [00:11<00:00, 99.3MB/s]\n",
            "✅ Dataset Ready! Vocab Size: 8781, Max Length: 37\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from google.colab import files, drive\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pickle\n",
        "\n",
        "# 1. Mount Drive and setup Path\n",
        "drive.mount('/content/drive')\n",
        "SAVE_PATH = '/content/drive/MyDrive/Flickr_Project/'\n",
        "if not os.path.exists(SAVE_PATH):\n",
        "    os.makedirs(SAVE_PATH)\n",
        "\n",
        "# 2. Dataset Download\n",
        "if not os.path.exists('kaggle.json'):\n",
        "    files.upload() # Upload your kaggle.json here\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d adityajn105/flickr8k\n",
        "!unzip -q flickr8k.zip -d flickr_data\n",
        "\n",
        "# 3. Process Captions\n",
        "df = pd.read_csv('flickr_data/captions.txt')\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = \"\".join([char for char in text if char.isalpha() or char.isspace()])\n",
        "    text = \" \".join(text.split())\n",
        "    return f\"startseq {text} endseq\"\n",
        "\n",
        "df['caption'] = df['caption'].apply(clean_text)\n",
        "\n",
        "# 4. Create Tokenizer (The Dictib onary)\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df['caption'].tolist())\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "max_length = max(len(c.split()) for c in df['caption'].tolist())\n",
        "\n",
        "# 5. Save Tokenizer immediately\n",
        "with open(SAVE_PATH + 'tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "print(f\"✅ Dataset Ready! Vocab Size: {vocab_size}, Max Length: {max_length}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f_byu_-nLN_",
        "outputId": "71eb0dae-d47f-4e8f-d86d-fd71334bd6ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "✅ Model Architecture Built & Compiled.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Add, GlobalAveragePooling2D\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "\n",
        "# ENCODER (Image Features)\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "base_model.trainable = False\n",
        "image_input = Input(shape=(224, 224, 3))\n",
        "x = base_model(image_input)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "image_features = Dense(256, activation='relu')(x)\n",
        "\n",
        "# DECODER (Sequence Processing)\n",
        "caption_input = Input(shape=(max_length,))\n",
        "y = Embedding(vocab_size, 256, mask_zero=False)(caption_input)\n",
        "y = LSTM(256)(y)\n",
        "\n",
        "# MERGE\n",
        "decoder = Add()([image_features, y])\n",
        "decoder = Dense(256, activation='relu')(decoder)\n",
        "output = Dense(vocab_size, activation='softmax')(decoder)\n",
        "\n",
        "model = Model(inputs=[image_input, caption_input], outputs=output)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "print(\"✅ Model Architecture Built & Compiled.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfr8u-cYnS-j",
        "outputId": "98d77d25-9ef7-4b50-c4f1-fe0883be6624"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⏳ Training starting...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1079s\u001b[0m 846ms/step - loss: 5.4735\n",
            "Epoch 2/20\n",
            "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m769s\u001b[0m 608ms/step - loss: 4.9500\n",
            "Epoch 3/20\n",
            "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m768s\u001b[0m 608ms/step - loss: 4.7926\n",
            "Epoch 4/20\n",
            "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m769s\u001b[0m 608ms/step - loss: 4.6794\n",
            "Epoch 5/20\n",
            "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m768s\u001b[0m 607ms/step - loss: 4.5897\n",
            "Epoch 6/20\n",
            "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m768s\u001b[0m 607ms/step - loss: 4.3332\n",
            "Epoch 7/20\n",
            "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m767s\u001b[0m 607ms/step - loss: 3.1557\n",
            "Epoch 8/20\n",
            "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m768s\u001b[0m 607ms/step - loss: 2.7789\n",
            "Epoch 9/20\n",
            "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m768s\u001b[0m 608ms/step - loss: 2.5712\n",
            "Epoch 10/20\n",
            "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m768s\u001b[0m 607ms/step - loss: 2.3971\n",
            "Epoch 11/20\n",
            "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m769s\u001b[0m 608ms/step - loss: 2.2941\n",
            "Epoch 12/20\n",
            "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m771s\u001b[0m 610ms/step - loss: 2.2067\n",
            "Epoch 13/20\n",
            "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m771s\u001b[0m 609ms/step - loss: 2.1184\n",
            "Epoch 14/20\n",
            "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m772s\u001b[0m 610ms/step - loss: 2.0486\n",
            "Epoch 15/20\n",
            "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m772s\u001b[0m 610ms/step - loss: 1.9795\n",
            "Epoch 16/20\n",
            "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m771s\u001b[0m 610ms/step - loss: 1.9250\n",
            "Epoch 17/20\n",
            "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m772s\u001b[0m 611ms/step - loss: 1.8738\n",
            "Epoch 18/20\n",
            "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m802s\u001b[0m 611ms/step - loss: 1.8226\n",
            "Epoch 19/20\n",
            "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m771s\u001b[0m 610ms/step - loss: 1.7769\n",
            "Epoch 20/20\n",
            "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m770s\u001b[0m 609ms/step - loss: 1.7288\n",
            "✅ Model and Tokenizer are now safely in: /content/drive/MyDrive/Flickr_Project/\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.utils import Sequence, to_categorical\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "class FlickrGenerator(Sequence):\n",
        "    def __init__(self, df, tokenizer, max_length, vocab_size, batch_size=32):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.batch_size = batch_size\n",
        "        self.img_dir = 'flickr_data/Images'\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch = self.df.iloc[index * self.batch_size : (index + 1) * self.batch_size]\n",
        "        X_img, X_txt, y_label = [], [], []\n",
        "        for _, row in batch.iterrows():\n",
        "            img_path = os.path.join(self.img_dir, row['image'])\n",
        "            img = load_img(img_path, target_size=(224, 224))\n",
        "            img = img_to_array(img) / 255.0\n",
        "            seq = self.tokenizer.texts_to_sequences([row['caption']])[0]\n",
        "            for i in range(1, len(seq)):\n",
        "                in_seq, out_seq = seq[:i], seq[i]\n",
        "                in_seq = pad_sequences([in_seq], maxlen=self.max_length, padding='post')[0]\n",
        "                out_seq = to_categorical([out_seq], num_classes=self.vocab_size)[0]\n",
        "                X_img.append(img)\n",
        "                X_txt.append(in_seq)\n",
        "                y_label.append(out_seq)\n",
        "        return (np.array(X_img), np.array(X_txt)), np.array(y_label)\n",
        "\n",
        "# 1. Initialize generator\n",
        "generator = FlickrGenerator(df, tokenizer, max_length, vocab_size, batch_size=32)\n",
        "\n",
        "# 2. Train (20 Epochs for good results)\n",
        "print(\"⏳ Training starting...\")\n",
        "model.fit(generator, epochs=20, verbose=1)\n",
        "\n",
        "# 3. Save to Drive Folder\n",
        "model.save(SAVE_PATH + 'flickr8k_model.keras')\n",
        "print(f\"✅ Model and Tokenizer are now safely in: {SAVE_PATH}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
